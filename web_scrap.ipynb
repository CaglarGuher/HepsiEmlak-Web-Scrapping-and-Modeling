{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "def url_to_info(url):\n",
    "\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "    adress_main = soup.find('ul', class_='short-info-list')\n",
    "    li_elements = adress_main.find_all('li')\n",
    "    adress_city = li_elements[0].get_text(strip=True)\n",
    "    adress_district = li_elements[1].get_text(strip=True)\n",
    "    adress_neighborhood= li_elements[2].get_text(strip=True)\n",
    "    price_element = soup.find('p', class_='fz24-text price').get_text(strip=True)\n",
    "\n",
    "    gross_m2 = soup.find_all('ul', class_='adv-info-list')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    data = {'City': adress_city, 'District': adress_district, 'Neighborhood': adress_neighborhood }\n",
    "\n",
    "\n",
    "\n",
    "    for ul in gross_m2:\n",
    "        li_elements = ul.find_all('li', {'class': 'spec-item'})\n",
    "        for li in li_elements:\n",
    "            try:\n",
    "                txt = li.find('span', {'class': 'txt'}).get_text(strip=True)\n",
    "                value = li.find_all('span', {'data-v-60d12482': ''})[1].get_text(strip=True)\n",
    "                data[txt] = value\n",
    "            except:\n",
    "                continue\n",
    "    data[\"Rent\"] = price_element\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_csv = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful\n",
      "last scrapped page is 1360\n",
      "Request successful\n",
      "last scrapped page is 1361\n",
      "Request successful\n",
      "last scrapped page is 1362\n",
      "Request successful\n",
      "last scrapped page is 1363\n",
      "Request successful\n",
      "last scrapped page is 1364\n",
      "Request successful\n",
      "last scrapped page is 1365\n",
      "Request successful\n",
      "last scrapped page is 1366\n",
      "Request successful\n",
      "last scrapped page is 1367\n",
      "Request successful\n",
      "last scrapped page is 1368\n",
      "Request successful\n",
      "last scrapped page is 1369\n",
      "Request successful\n",
      "last scrapped page is 1370\n",
      "Request successful\n",
      "last scrapped page is 1371\n",
      "Request successful\n",
      "last scrapped page is 1372\n",
      "Request successful\n",
      "last scrapped page is 1373\n",
      "Request successful\n",
      "last scrapped page is 1374\n",
      "Request successful\n",
      "last scrapped page is 1375\n",
      "Request successful\n",
      "last scrapped page is 1376\n",
      "Request successful\n",
      "last scrapped page is 1377\n",
      "Request successful\n",
      "last scrapped page is 1378\n",
      "Request successful\n",
      "last scrapped page is 1379\n",
      "Request successful\n",
      "last scrapped page is 1380\n",
      "Request successful\n",
      "last scrapped page is 1381\n",
      "Request successful\n",
      "last scrapped page is 1382\n",
      "Request successful\n",
      "last scrapped page is 1383\n",
      "Request successful\n",
      "last scrapped page is 1384\n",
      "Request successful\n",
      "last scrapped page is 1385\n",
      "Request successful\n",
      "last scrapped page is 1386\n",
      "Request successful\n",
      "last scrapped page is 1387\n",
      "Request successful\n",
      "last scrapped page is 1388\n",
      "Request successful\n",
      "last scrapped page is 1389\n",
      "Request successful\n",
      "last scrapped page is 1390\n",
      "Request successful\n",
      "last scrapped page is 1391\n",
      "Request successful\n",
      "last scrapped page is 1392\n",
      "Request successful\n",
      "last scrapped page is 1393\n",
      "Request successful\n",
      "last scrapped page is 1394\n",
      "Request successful\n",
      "last scrapped page is 1395\n",
      "Request successful\n",
      "last scrapped page is 1396\n",
      "Request successful\n",
      "last scrapped page is 1397\n",
      "Request successful\n",
      "last scrapped page is 1398\n",
      "Request successful\n",
      "last scrapped page is 1399\n",
      "Request successful\n",
      "last scrapped page is 1400\n",
      "Request successful\n",
      "last scrapped page is 1401\n",
      "Request successful\n",
      "last scrapped page is 1402\n",
      "Request successful\n",
      "last scrapped page is 1403\n",
      "Request successful\n",
      "last scrapped page is 1404\n",
      "Request successful\n",
      "last scrapped page is 1405\n",
      "Request successful\n",
      "last scrapped page is 1406\n",
      "Request successful\n",
      "last scrapped page is 1407\n",
      "Request successful\n",
      "last scrapped page is 1408\n",
      "Request successful\n",
      "last scrapped page is 1409\n",
      "Request successful\n",
      "last scrapped page is 1410\n",
      "Request successful\n",
      "last scrapped page is 1411\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i in range (1360,1412):\n",
    "\n",
    "    \n",
    "    \n",
    "    url = f\"https://www.hepsiemlak.com/ankara-satilik?page={i}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        print('Request successful')\n",
    "    elif response.status_code == 403:\n",
    "        print(f'Request forbitten with status code {response.status_code}')\n",
    "        pass\n",
    "    else:\n",
    "        print(f'too many request  with status code {response.status_code}')\n",
    "        pass\n",
    "\n",
    "    time.sleep(40)\n",
    "\n",
    "    ## I have put time because website had request limit\n",
    "    content = response.content\n",
    "\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "    value = soup.find_all(\"div\",class_ =\"links\")\n",
    "\n",
    "    for value in value:\n",
    "        link_elements = value('a', class_='card-link')\n",
    "\n",
    "        links = []\n",
    "        for link_element in link_elements:\n",
    "            href = link_element['href']\n",
    "            full_url = f'https://www.hepsiemlak.com{href}'\n",
    "            links.append(full_url)\n",
    "\n",
    "        # Print the links\n",
    "        for link in links:\n",
    "            \n",
    "            data = url_to_info(link)\n",
    "\n",
    "            total_csv.append(data)\n",
    "\n",
    "    print(f\"last scrapped page is {i}\")\n",
    "    import pandas as pd \n",
    "    total_data =pd.DataFrame(total_csv)\n",
    "    total_data.to_csv(\"house_prices_v5.csv\")\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the webscrapping part, I did not spend too much time to make the code clean, I did not\n",
    "## cover the times the internet went down , so  I just printed the scrapped pages and when int connection is lost, ı rerun the loop\n",
    "## for the remaining part and then merge them all \n",
    "\n",
    "part1 = pd.read_csv(\"house_price.csv\")\n",
    "part2 = pd.read_csv(\"house_prices_v1.csv\")\n",
    "part3 = pd.read_csv(\"house_prices_v2.csv\")\n",
    "part4 =pd.read_csv(\"house_prices_v3.csv\")\n",
    "part5 =pd.read_csv(\"house_prices_v4.csv\")\n",
    "part6 = pd.read_csv(\"house_prices_v5.csv\")\n",
    "part7 =pd.read_csv(\"house_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = [part1,part2,part3,part4,part5,part6,part7]\n",
    "total_dataframe = pd.concat(total,axis = 0)\n",
    "total_dataframe = total_dataframe.reset_index()\n",
    "total_dataframe = total_dataframe.drop(total_dataframe.columns[0],axis=1)\n",
    "total_dataframe = total_dataframe.drop(total_dataframe.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                         0\n",
       "District                     0\n",
       "Neighborhood                 0\n",
       "İlan no                      0\n",
       "Son Güncelleme Tarihi        0\n",
       "Oda + Salon Sayısı           0\n",
       "Brüt / Net M2                0\n",
       "Bulunduğu Kat             1773\n",
       "Bina Yaşı                    0\n",
       "Isınma Tipi                  0\n",
       "Kat Sayısı                  26\n",
       "Krediye Uygunluk             4\n",
       "Eşya Durumu               2051\n",
       "Banyo Sayısı                22\n",
       "Kullanım Durumu           1951\n",
       "Takas                     3313\n",
       "Cephe                     4624\n",
       "Rent                         0\n",
       "Tapu Durumu               3574\n",
       "Yapı Tipi                 7256\n",
       "Yapının Durumu            6698\n",
       "Aidat                    14733\n",
       "Kira Getirisi            16246\n",
       "Yakıt Tipi                5771\n",
       "Yetkili Ofis              9567\n",
       "Site İçerisinde          30259\n",
       "Depozito                 33322\n",
       "Kapalı Alan M2           33325\n",
       "Açık Alan M2             33325\n",
       "Bina Sayısı              33325\n",
       "Ada No                   33324\n",
       "Parsel No                33324\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dataframe.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
