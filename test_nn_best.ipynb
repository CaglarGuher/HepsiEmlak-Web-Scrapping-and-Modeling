{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torch import nn,optim\n",
    "from ray.air import session\n",
    "import pandas as pd\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "import ray\n",
    "from ray import  tune\n",
    "from ray.air import session\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from hyperopt import hp\n",
    "import torch.nn as nn\n",
    "from data_proc import normalized_dataset_test_0,normalized_dataset_train_0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19480536337941884\n",
      "0.07718850998624936\n",
      "0.0658252226397284\n",
      "0.17459486588946918\n",
      "0.16917814746387858\n",
      "0.03615961024579384\n",
      "0.07611925984218494\n",
      "0.11299338337840206\n",
      "0.0924734383599082\n",
      "0.22519922340983775\n",
      "0.08531427145866094\n",
      "0.2803699368193968\n",
      "0.14834721132612494\n",
      "0.06530961287694413\n",
      "0.10841692137434635\n",
      "0.22689395521336528\n",
      "0.19239222733720915\n",
      "0.10872279042239065\n",
      "0.18333237376898082\n",
      "0.13643313391940362\n",
      "0.05213250436528183\n",
      "0.1064069890347657\n",
      "0.16336738059102598\n",
      "0.25544883754493586\n",
      "0.15576234493793836\n",
      "0.15649893030857862\n",
      "0.125283076842107\n",
      "0.12167660648753839\n",
      "0.08011910661693607\n",
      "0.3108891528266579\n",
      "0.16941497506601597\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PC\\Desktop\\roketsan_data_ML\\rok_ML\\test_nn_best.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/roketsan_data_ML/rok_ML/test_nn_best.ipynb#W2sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m         output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/roketsan_data_ML/rok_ML/test_nn_best.ipynb#W2sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/roketsan_data_ML/rok_ML/test_nn_best.ipynb#W2sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/roketsan_data_ML/rok_ML/test_nn_best.ipynb#W2sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/roketsan_data_ML/rok_ML/test_nn_best.ipynb#W2sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m#####TRAINING LOOP#######    \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/roketsan_data_ML/rok_ML/test_nn_best.ipynb#W2sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m###EVALUATION####       \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\machine_learn\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\machine_learn\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class AdaptiveTanh(torch.nn.Module):\n",
    "    def __init__(self, init_alpha=0.5, init_beta=0.5):\n",
    "        self.alpha = torch.nn.Parameter(torch.tensor(init_alpha))\n",
    "        self.beta = torch.nn.Parameter(torch.tensor(init_beta))\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.alpha*x + self.beta) \n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,L1,L2,L3,a,b,g,d):\n",
    "        super(Net,self).__init__()\n",
    "  \n",
    "        self.layers = nn.Sequential(\n",
    "      nn.Linear(6,L1),\n",
    "      AdaptiveTanh(init_alpha=a, init_beta=b, init_gamma=g, init_delta=d),\n",
    "      nn.Linear(L1,L2),\n",
    "      AdaptiveTanh(init_alpha=a ,init_beta=b, init_gamma=g, init_delta=d),\n",
    "      nn.Linear(L2,L3),\n",
    "      AdaptiveTanh(init_alpha=a, init_beta=b, init_gamma=g, init_delta=d),\n",
    "      nn.Linear(L3,1),\n",
    "      AdaptiveTanh(init_alpha=a, init_beta=b, init_gamma=g, init_delta=d),\n",
    "    )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "        \n",
    "      \n",
    "    \n",
    "\n",
    "model = Net(space[\"L1\"],space[\"L2\"],space[\"L3\"],space[\"a\"],space[\"b\"],space[\"g\"],space[\"d\"])\n",
    "#####MODEL#####\n",
    "\n",
    "\n",
    "normalized_train = DataLoader(normalized_dataset_train_0, batch_size=space[\"batch_size\"], shuffle=True)\n",
    "normalized_test = DataLoader(normalized_dataset_test_0, batch_size=space[\"batch_size\"], shuffle=False)\n",
    "\n",
    "###PARAMETERS#####\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = getattr(optim, space[\"optimizer\"])(model.parameters(), lr=0.001, eps=space[\"eps\"])\n",
    "num_epochs = 100\n",
    "###PARAMETERS#####\n",
    "\n",
    "\n",
    "\n",
    "#####TRAINING LOOP#######\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(normalized_train):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#####TRAINING LOOP#######    \n",
    "###EVALUATION####       \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(normalized_test):\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "        test_loss /= len(normalized_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21298bdcd0860ab4599171f42c3ab1efbee8dac89f062d99ffb3db1b36fddaa8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
