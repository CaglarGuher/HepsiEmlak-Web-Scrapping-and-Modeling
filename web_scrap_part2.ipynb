{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "\n",
    "\n",
    "def url_to_info(url):\n",
    "\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    content = response.content\n",
    "    \n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "    adress_main = soup.find('ul', class_='short-info-list')\n",
    "    li_elements = adress_main.find_all('li')\n",
    "    adress_city = li_elements[0].get_text(strip=True)\n",
    "    adress_district = li_elements[1].get_text(strip=True)\n",
    "    adress_neighborhood= li_elements[2].get_text(strip=True)\n",
    "    price_element = soup.find('p', class_='fz24-text price').get_text(strip=True)\n",
    "\n",
    "    gross_m2 = soup.find_all('ul', class_='adv-info-list')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    data = {'City': adress_city, 'District': adress_district, 'Neighborhood': adress_neighborhood }\n",
    "\n",
    "\n",
    "\n",
    "    for ul in gross_m2:\n",
    "        li_elements = ul.find_all('li', {'class': 'spec-item'})\n",
    "        for li in li_elements:\n",
    "            try:\n",
    "                txt = li.find('span', {'class': 'txt'}).get_text(strip=True)\n",
    "                value = li.find_all('span', {'data-v-60d12482': ''})[1].get_text(strip=True)\n",
    "                data[txt] = value\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "             \n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(5)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)\n",
    "    distance_elements = driver.find_elements(By.CLASS_NAME,\"whats-near-card-item\")\n",
    "    for element in distance_elements:\n",
    "\n",
    "    \n",
    "    # Extract the inner HTML of the element\n",
    "        html_content = element.get_attribute('innerHTML')\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find all the div elements with the class \"whats-near-card-item__description\"\n",
    "        description_divs = soup.find_all('div', class_='whats-near-card-item__description')\n",
    "        \n",
    "    # Find all the div elements with the class \"whats-near-card-item__distance\"\n",
    "        distance_divs = soup.find_all('div', class_='whats-near-card-item__distance')\n",
    "\n",
    "    # Iterate over the description divs and corresponding distance divs\n",
    "        for description_div, distance_div in zip(description_divs, distance_divs):\n",
    "            # Extract the name and value\n",
    "            name = description_div.span.get_text(strip=True)\n",
    "            value_for_name = distance_div.span.get_text(strip=True)\n",
    "            name_parts = name.split(\" - \")\n",
    "            name = name_parts[0]\n",
    "            data[name] = value_for_name\n",
    "            # Print the name and value\n",
    "            \n",
    "    data[\"Rent\"] = price_element\n",
    "\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_csv = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range (35,1000):\n",
    "\n",
    "    \n",
    "    \n",
    "    url = f\"https://www.hepsiemlak.com/ankara-satilik?page={i}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        print('Request successful')\n",
    "    elif response.status_code == 403:\n",
    "        print(f'Request forbitten with status code {response.status_code}')\n",
    "        pass\n",
    "    else:\n",
    "        print(f'too many request  with status code {response.status_code}')\n",
    "        pass\n",
    "\n",
    "    time.sleep(25)\n",
    "\n",
    "    ## I have put time because website had request limit\n",
    "    content = response.content\n",
    "\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "    value = soup.find_all(\"div\",class_ =\"links\")\n",
    "\n",
    "    for value in value:\n",
    "        link_elements = value('a', class_='card-link')\n",
    "\n",
    "        links = []\n",
    "        for link_element in link_elements:\n",
    "            href = link_element['href']\n",
    "            full_url = f'https://www.hepsiemlak.com{href}'\n",
    "            links.append(full_url)\n",
    "\n",
    "        for link in links:\n",
    "            \n",
    "            data = url_to_info(link)\n",
    "\n",
    "            total_csv.append(data)\n",
    "\n",
    "    print(f\"last scrapped page is {i}\")\n",
    "    import pandas as pd \n",
    "    total_data =pd.DataFrame(total_csv)\n",
    "    total_data.to_csv(\"house_prices_detailed2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
